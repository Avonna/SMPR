

# Теория машинного обучения #

## Содержание
* [Метрические алгоритмы классификации](#метрические-алгоритмы-классификации) 
	* [Метод *k* ближайших соседей *(kNN)*](#метод-k-ближайших-соседей-knn)  
	* [Метод *k* взвешенных ближайших соседей (*kwNN*)](#метод-k-взвешенных-ближайших-соседей-kwnn)  
	* [Метод парзеновского окна](#метод-парзеновского-окна)
	* [Метод потенциальных функций](#метод-потенциальных-функций)
* [Байесовские алгоритмы классификации](#байесовские-алгоритмы-классификации)
	* [Нормальный дискриминантный анализ](#нормальный-дискриминантный-анализ)
	* [Наивный байесовский классификатор](#наивный-байесовский-классификатор)
	* [Подстановочный алгоритм (Plug-in)](#plug-in)
	* [ЛДФ](#лдф)
	* [Сравнение plug-in и ЛДФ](#сравнение-plug-in-и-ЛДФ)
* [Линейные алгоритмы классификации](#линейные-алгоритмы-классификации)
	* [Стохастический градиент](#стохастический-градиент)
	* [ADALINE](#adaline)
	* [Персептрон Розенблатта](#персептрон-розенблатта)
	* [Логистическая регрессия](#логистическая-регрессия)
	* [Сравнение классификаторов](#сравнение-классификаторов)

#### Сводная таблица для метрических методов классификации ####

| *Метод*    							|*Оптимальный параметр*	| *LOO*, Скользящий контроль 	|*LOO, %*| 
| --------------------------------------------------------------|--------------------: 	| --------------------------:	| -----: | 
| [1NN](#метод-k-ближайших-соседей-knn)   			|  k = 1   		|   -  				|    -   |
| [kNN](#метод-k-ближайших-соседей-knn)   			|  k = 6   		|   5  				|  3.33  |
| [kwNN](#метод-k-взвешенных-ближайших-соседей-kwnn) 		|  k = 6, q = 0.1	|   5  				|  3.33  | 
| [Парзеновского окна, ядро Епанечникова](#метод-парзеновского-окна) | h = 0.4  	|   6   			|    4   |
| [Парзеновского окна, ядро Квартическое](#метод-парзеновского-окна) | h = 0.4  	|   6   			|    4   |
| [Парзеновского окна, ядро Треугольное](#метод-парзеновского-окна)  | h = 0.4  	|   6   			|    4   |
| [Парзеновского окна, ядро Прямоугольное](#метод-парзеновского-окна)| h = 0.4  	|   6   			|    4   |
| [Парзеновского окна, ядро Гауссовское](#метод-парзеновского-окна)  | h = 0.1  	|   6   			|    4   |

## Метрические алгоритмы классификации ##
Метрические алгоритмы классификации - алгоритмы, основанные на вычислении оценок сходства между объектами на основе весовых функций.
### Метод *k* ближайших соседей (*kNN*) ###
Один из наиболее простых метрических алгоритмов классификации.
Работает следующим образом: дан классифицируемый объект *z* и обучающая выборка ![](http://latex.codecogs.com/gif.latex?%24X%5El%24). Требуется определить класс объекта *z* на основе данных из обучающей выборки. Для этого:
1. Вся выборка ![](http://latex.codecogs.com/gif.latex?%24X%5El%24) сортируется по возрастанию расстояния от объекта *z* до каждого объекта выборки.
2. Проверяются классы *k* ближайших соседей объекта *z*. Класс, встречаемый наиболее часто среди *k* соседей, присваивается объекту *z*.  

Исходными данными для алгоритма являются: классифицируемый объект, обучающая выборка и параметр *k* - число рассматриваемых ближайших соседей.
Результатом работы метода является класс классифицируемого объекта.

При *k = 1* алгоритм превращается в частный случай *1NN*. В таком слачае, рассматриваемый объект *z* присваивается к классу его первого ближайшего соседа. В свою очередь, остальные объекты не рассматриваются.

Пример работы *1NN* при использовании в качестве обучающей выборки Ирисы Фишера:

![1NN.png](https://github.com/VladislavDuma/SMPR/blob/master/img/1nn_allelem_2.png)

Для проверки оптимальности *k* используется Критерий Скользящего Контроля *LOO* (Leave One Out).
Данный критерий проверяет оптимальность значения *k* следующим образом:
1. Из обучающей выборки удаляется *i*-й объект ![](http://latex.codecogs.com/gif.latex?%24x%5Ei%24).
2. Запоминаем "старый" класс *i*-го объекта.
3. Запускаем алгоритм для оставшейся выборки. В результате работы *i*-му элементу присваивается "новый" класс на основе имеющейся выборки. Если значения "нового" и "старого" класса совпали, то *i*-ый элемент классифицировало верно. При их же несовпадении сумма ошибки увеличивается на 1.
4. Шаги 1-3 повторяются для каждого объекта выборки при фиксированном *k*. По окончании работы алгоритма полученная сумма ошибки *sum* делится на размер выборки *l*: ![sum=sum/l](http://latex.codecogs.com/gif.latex?sum%3D%20%5Cfrac%7Bsum%7D%7Bl%7D) .  Потом значение *k* меняется, и алгоритм повторяется для нового значения. *k* с наименьшим значением суммы ошибки будет оптимальным.
#### Реализация kNN
При реализации алгоритма, в качестве обучающей выборки использовалась выборка ирисов Фишера. В качестве признаков объектов использовались значения длины и ширины лепестка. Значение *k* подбиралось по *LOO*.

Алгоритм:

    kNN <- function(xl, k, z) {
	  orderedXL <- sortObjectByDist(xl, z)
	  n <- dim(orderedXL)[2]
	  classes <- orderedXL[1:k, n] 
	  counts <- table(classes) # Таблица встречаемости каждого класса среди k ближайших соседей объекта
	  class <- names(which.max(counts)) # Наиболее часто встречаемый класс
	  return (class)
	}
где *xl* - обучающая выборка.

Пример работы метода kNN при k = 10 даёт следующий результат.

![kNN.png](https://github.com/VladislavDuma/SMPR/blob/master/img/kNN_10elem_2.png)

Применив критерий *LOO* для получения оптимального *k* мы получаем следующий ответ:

![LOO_for_kNN.png](https://github.com/VladislavDuma/SMPR/blob/master/img/LOO_for_kNN_3.png)

Следовательно оптимальным *k* на выборке Ирисы Фишера явлется значение *k = 6*. Построим графики *kNN*:

![kNN_k6.png](https://github.com/VladislavDuma/SMPR/blob/master/img/kNN_k6_v1.png)

Достоинства алгоритма:
1. Простота реализации (относительно)
2. Хорошее качество, при правильно подобранной метрике и параметре *k*

Недостатки алгоритма:
1. Необходимость хранить выборку целиком, как следствие - неэффективное использование памяти
2. Малый набор параметров
3. Качество классификации сильно зависит от выбранной метрики
4. "Выбросы" могут значительно ухудшить точность

[К началу алгоритма (*kNN*)](#метод-k-ближайших-соседей-knn)

[Вернуться к содержанию](#содержание)

### Метод *k* взвешенных ближайших соседей (*kwNN*) ###
Метод *kwNN* отличается от *kNN* тем, что вес ближайших соседей зависит не от ранга соседа, а от расстояния до объекта z. В методе *kNN* считается, что вес каждого *k*-соседа равен 1. По сути, мы считали частоту появления классов среди ближайших *k* соседей.
Применяя же метод *kwNN* в качестве весовой функции мы используем *w* = *q^i*, что соответствует методу *k* экспоненциально взвешенных ближайших соседей. Предполагается, что *q* принадлежит [0.1; 1.0] и рассматриваем с шагом 0.1.

#### Реализация kwNN

Алгоритм:

	kwNN <- function(xl, z, k, q)
	{
	  orderedXl <- sortObjectByDist(xl, z)	# сортируем выборку
	  n <- dim(xl)[2]			# размерность выборки по столбцам

	  classes <- orderedXl[1:k, n] 		# берём k ближайших соседей

	  classes <- table(classes)    		# создаём таблицу для них
	  classes[1:length(classes)] <- 0	# обнуляем значения
	  
	  for (i in names(classes)) {		# для всех классов
	    for (j in 1:k) {			# для всех значений k
	      if (orderedXl[j, n] == i)		
		classes[i] = classes[i] + q^j	# суммируем веса для всех объектов одного класса
	    }
	  }
	  class <- names(which.max(classes))	# возвращаем класс соответствующий максимальному весу
	  return (class)
	}
	
Для выборки Ирисы Фишера получаем следующий результат:

![kwNN.png](https://github.com/VladislavDuma/SMPR/blob/master/img/kwNN_v1.png)

В результате работы мы получили, что наиболее оптимальными явлеются значения *k = 6*.

Достоинства алгоритма:
1. Простота реализации
2. Хорошее качество, при правильно подобранной метрике и параметрах *k* и *q*

Недостатки алгоритма:
1. Необходимость хранить выборку целиком, как следствие - неэффективное использование памяти
2. Чрезмерное усложнение решающего правила
3. Качество классификации сильно зависит от выбранной метрики
4. Поиск ближайшего соседа предполагает сравнение классифицируемого объекта со всеми объектами выборки, что требует линейного по длине выборки числа операций.

[К началу алгоритма (*kwNN*)](#метод-k-взвешенных-ближайших-соседей-kwnn)

[Вернуться к содержанию](#содержание)

### Метод парзеновского окна ###
Основное отличие метода парзеновского окна от метода ближайших соседей заключается в том, что весовую функцию мы рассматриваем не как ранговую, а как функцию расстояния.

Функция веса в таком случае выглядит следующим образом:

![](http://latex.codecogs.com/gif.latex?w%28i%2C%20z%29%20%3D%20K%28%5Cfrac%7B%5Crho%28z%2C%20x_%7Bi%7D%29%7D%7Bh%7D%29)

Где i - номер объекта выборки, z - классифицируемый объект, xi - i-й объект выборки, h - ширина окна, K - функция ядра.

Функция ядра - произвольная чётная функция, невозрастающая на *[0, +inf)*. На практике применяются следующие функции ядра:
1.  ![](http://latex.codecogs.com/gif.latex?K%28r%29%20%3D%20E%28r%29%20%3D%20%5Cfrac%7B3%7D%7B4%7D%281%20-%20r%5E%7B2%7D%29%5B%7Cr%7C%3C%3D1%5D) - Ядро Епанечникова
2.  ![](http://latex.codecogs.com/gif.latex?K%28r%29%20%3D%20Q%28r%29%20%3D%20%5Cfrac%7B15%7D%7B16%7D%281%20-%20r%5E%7B2%7D%29%5E%7B2%7D%5B%7Cr%7C%3C%3D1%5D) - Квартическое ядро
3.  ![](http://latex.codecogs.com/gif.latex?K%28r%29%20%3D%20T%28r%29%20%3D%20%281%20-%20%7Cr%7C%29%5B%7Cr%7C%3C%3D1%5D) - Треугольное ядро
4.  ![](http://latex.codecogs.com/gif.latex?K%28r%29%20%3D%20P%28r%29%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5B%7Cr%7C%20%3C%3D%201%5D) - Прямоугольное ядро
5.  ![](http://latex.codecogs.com/gif.latex?K%28r%29%20%3D%20G%28r%29%20%3D%20%282%5Cpi%29%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7De%5E%7B%28-%5Cfrac%7B1%7D%7B2%7Dr%5E%7B2%7D%29%7D) - Гауссовское ядро

Где: ![](http://latex.codecogs.com/gif.latex?r%3D%5Cfrac%7B%5Crho%28z%2C%20x_%7Bi%7D%29%7D%7Bh%7D)

#### Реализация ####

Алгоритм:

	parzen <- function(xl, h, distances, type_core) {
  	  # Оценка весовой функции по расстоянию, а не по рангу 
	  # h - ширина окна
	  # distances - расстояния от точки z до каждого объекта из выборки xl 
	  # type_core - функция ядра
	  l <- nrow(xl) # строки
	  n <- ncol(xl) # столбцы (размерность)

	  classes <- xl[1:l, n] # Классы объектов выборки
	  weights <- table(classes) # Таблица весов классов
	  weights[1:length(weights)] <- 0

	  for (i in 1:l) { # Для всех объектов выборки
	    class <- xl[i, n] # Берём его класс
	    r <- distances[i] / h
	    weights[class] <- weights[class] + type_core(r) # И прибавляем его вес к общему весу его класса
	  }

	  if (max(weights) != 0) # Если веса точки по классам не равны 0 (точка попала в окно)
	    return (names(which.max(weights))) # Вернуть класс с максимальным весом
	  else
	    return (0) # Иначе - вернуть 0
	}

В результате работы, при *h* [0.1; 2.0] с шагом 0.1, на выборке ирисов Фишера мы получили следующие результаты:

*Ядро Епанечникова*
![parz_ep.png](https://github.com/VladislavDuma/SMPR/blob/master/img/parzen_Ep.png)

*Квартическое ядро*
![parz_qt.png](https://github.com/VladislavDuma/SMPR/blob/master/img/parzen_Qart.png)

*Треугольное ядро*
![parz_tr.png](https://github.com/VladislavDuma/SMPR/blob/master/img/parzen_trian.png)

*Гауссовское ядро*
![parz_gs.png](https://github.com/VladislavDuma/SMPR/blob/master/img/Parzen_Gauss.png)

*Прямоугольное ядро*
![parz_gs.png](https://github.com/VladislavDuma/SMPR/blob/master/img/parzen_Pr.png)

#### Сводная таблица для алгоритма парзеновского окна ####

| *Тип ядра*    | *h*  | *LOO* | *LOO, %* |
| ------------- |----- | -----:| -------: |
| Епанечникова  | 0.4  |   6   |    4  	  |
| Квартическое  | 0.4  |   6   |    4     |
| Треугольное   | 0.4  |   6   |    4     |
| Прямоугольное | 0.4  |   6   |    4     |
| Гауссовское   | 0.1  |   6   |    4     |

Достоинства:
1. Простота реализации
2. Хорошее качество классификации при правильно подобранном *h*
3. Не требуется сортировка выборки, в отличии от метода ближайших соседей, что существенно ускоряет время классификации
4. Выбор ядра слабо влияет на качество классификации.

Недостатки:
1. Необходимо хранить выборку целиком
2. Слишком узкие окна *h* приводят к неустойчивой классификации; а слишком широкие - к вырождению алгоритма в константу
3. Если ни один объект выборки не попал в радиус окна *h* вокруг объекта *z*, то алгоритм не способен его проклассифицировать. Для обхода этого недостатка используется окно *h* переменной ширины

[К началу алгоритма (парзеновского окна)](#метод-парзеновского-окна)

[Вернуться к содержанию](#содержание)

### Метод потенциальных функций ###

*Метод потенциальных функций* - метрический классификатор, частный случай метода ближайших соседей. Позволяет с помощью простого алгоритма оценивать вес («важность») объектов обучающей выборки при решении задачи классификации.

#### Описание алгоритма ####

Дана обучающая выборка ![](http://latex.codecogs.com/gif.latex?%24X%5El%24) и объект *z*, который требуется классифицировать

1. Для каждого элемента из выборки задаётся ширина окна *h*
2. Для каждого элемента выборки задаётся сила потенциала *Y*
3. Каждому объекту выборки задаётся вес функции *W*
4. Суммируем веса объектов одинаковых классов. Класс с наибольшим весом присваиваетс объекту *z*.

Метод подбора *Y*

1. Задаём изначально все потенциалы равными 0 и максимальное количество ошибок.
2. Выборки последовательно или случайным образом выьирается объект из выборки *Xi*.
3. Для *Xi* запускается алгоритм классификации. Если полученный в результате класс не соответствует исходному, то сила поенциала данного объекта увеличивается на 1. В ином случае повторяем предыдущие шаги.
4. Алгоритм классификации с полученными значениями потенциалов запускается для каждого объекта выборки. Подсчитываем количество ошибок.
5. Если число ошибок меньше заданного (*eps*), то алгоритм завершает работу. В ином случае повторяем шаги 2-6.

Алгоритм метода потенциальных функций:

	potentialFunction <- function(distances, potentials, h, xl, type_core) {
	  l <- nrow(xl)
	  n <- ncol(xl)
	  classes <- xl[, n]
	  weights <- table(classes) # Таблица для весов классов
	  weights[1:length(weights)] <- 0 # По умолчанию все веса равны нулю
	  for (i in 1:l) { # Для каждого объекта выборки
	    class <- xl[i, n] # Берется его класс
	    r <- distances[i] / h[i]
	    weights[class] <- weights[class] + potentials[i] * type_core(r) # Считается его вес, и прибавляется к общему ввесу его класса
	  }
	  if (max(weights) != 0) return (names(which.max(weights))) # Если есть вес больше нуля, то вернуть класс с наибольшим весом
	  return (0) # Если точка не проклассифицировалась вернуть 0
	}

Алгоритм поиска потенциала:

	getPotentials <- function(xl, h, eps, type_core)
	{
	  l <- nrow(xl) # строки
	  n <- ncol(xl) # столбцы (размерность)
	  potentials <- rep(0, l)
	  distances_to_points <- matrix(0, l, l)
	  err <- eps + 1 # будем считать количество ошибок на выборке 
	  # err = (eps + 1) для предотвращения раннего выхода из цикла
	  # матрица дистанций до других точек выборки
	  for (i in 1:l)
	    distances_to_points[i,] <- getDistances(xl, c(xl[i, 1], xl[i, 2])) 
	  # xl - выборка, xl[i] - текущая точка и её координата
	  while(err > eps){
	    while (TRUE) {
	      # Продолжаем, пока не встретим ошибочное определение к классу
	      cur <- sample(1:l, 1) # выбираем случайную точку из выборки
	      class <- potentialFunction(distances_to_points[cur, ], potentials, h, xl, type_core)

	      if (class != xl[cur, n]) { # если не соответсвует классу
		potentials[cur] = potentials[cur] + 1 # увеличиваем потенциал
		break
	      } #if
	    } #while(true)

	    # считаем количество ошибок
	    err <- 0
	    for (i in 1:l) {
	      class <- potentialFunction(distances_to_points[i, ], potentials, h, xl, type_core)
	      err <- err + (class != xl[i, n])
	    }
	  }
	  return (potentials)
	}

Примеры работы метода потенциальных функций на разных типах ядра:

*Ядро Епанечникова*
![potential_epanech_core.png](https://github.com/VladislavDuma/SMPR/blob/master/img/potential_epanech_core_v2.png)

*Квартическое ядро*
![potential_quarter_core.png](https://github.com/VladislavDuma/SMPR/blob/master/img/potential_quarter_core.png)

Достоинства:
1. Простота реализации
2. Хранит лишь часть выборки, следовательно, экономит память

Недостатки:
1. Порождаемый алгоритм медленно сходится
2. Параметры настраиваются слишком грубо
3. При случайном подборе *Y* время работы может сильно отличаться

[К началу алгоритма (потенциальных функций)](#метод-потенциальных-функций)

[Вернуться к содержанию](#содержание)

## Байесовские алгоритмы классификации ##

### Нормальный дискриминантный анализ ###

Нормальный дискриминантный анализ - это специальный случай байесовского классификации, когда предполагается, что плотности всех классов  являются многомерными нормальными.

Функция плотности многомерного нормального распределения выглядит следующим образом:

![](http://latex.codecogs.com/gif.latex?N%28x%2C%20%5Cmu%2C%20%5CSigma%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%28%282%5Cpi%29%5E%7Bn%7D%7C%5CSigma%7C%29%7De%5E%7B-%5Cfrac%7B1%7D%7B2%7D%28%28x-%5Cmu%29%5CSigma%5E%7B-1%7D%28x-%5Cmu%29%5E%7BT%7D%29%7D), где 

![](http://latex.codecogs.com/gif.latex?x%20%5Cin%20R%5E%7Bn%7D) - объект, состоящий из *n* признаков  
![](http://latex.codecogs.com/gif.latex?%5Cmu%20%5Cin%20R%5E%7Bn%7D) - мат. ожидание каждого признака  
![](http://latex.codecogs.com/gif.latex?%5CSigma%20%5Cin%20R%5E%7Bn%5Ctimes%20n%7D) - матрица ковариации признаков. Симметричная, невырожденная, положительно определённая.

*Примеры работы программы на Shiny*

1. Если признаки некоррелированы, т. е. матрица ковариации диагональна, то линии уровня имеют форму эллипсоидов, параллельных осям координат, вытянутых относительно признака, значение для которого в матрице выше.

![level_lines_7.PNG](https://github.com/VladislavDuma/SMPR/blob/master/img/level_lines/level_lines_7.PNG)

![level_lines_5.PNG](https://github.com/VladislavDuma/SMPR/blob/master/img/level_lines/level_lines_5.PNG)

![level_lines_2.PNG](https://github.com/VladislavDuma/SMPR/blob/master/img/level_lines/level_lines_2.PNG)

![level_lines_1.PNG](https://github.com/VladislavDuma/SMPR/blob/master/img/level_lines/level_lines_1.PNG)

2. Если признаки коррелированы, то есть матрица ковариации не диагональна, то линии уровня имеют форму эллипсоидов, наклонённых относительно осей координат.

![level_lines_3.png](https://github.com/VladislavDuma/SMPR/blob/master/img/level_lines/level_lines_3.PNG)

![level_lines_6.png](https://github.com/VladislavDuma/SMPR/blob/master/img/level_lines/level_lines_6.PNG)

![level_lines_4.png](https://github.com/VladislavDuma/SMPR/blob/master/img/level_lines/level_lines_4.PNG)

Работу программы с реализацией на Shiny (линии уровня) можно посмотреть здесь [здесь](https://vladislav-duma.shinyapps.io/level_lines/)

[Вернуться к содержанию](#содержание)

### Наивный байесовский классификатор ###
Если все признаки объектов выборки сформированы независимо, то значение функции плотности представляется в виде ![](http://latex.codecogs.com/gif.latex?p_%7By%7D%28x%29%3Dp_%7By%7D%28%5Cxi_%7B1%7D%29*%5Cdots*p_%7By%7D%28%5Cxi_%7Bn%7D%29), где ![](http://latex.codecogs.com/gif.latex?%5Cxi_%7Bi%7D) - *i*-й признак объекта *x*. Это значительно упрощает задачу, так как оценивать несколько одномерных плотностей легче, чем одну многомерную.  
Но на практике подобная ситуация встречается редко. Это одна из причин, по которой алгоритм получил название "Наивный байесовский классификатор". Обычно он используется, как эталон при сравнении различных алгоритмов классификации.  
В данном случае решающее правило принимает вид:  
![](http://latex.codecogs.com/gif.latex?a%28x%29%3Darg%20%5Cmax_%7By%5Cin%20Y%7D%28%5Cln%28%5Clambda_%7By%7DP_y%29&plus;%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%5Cln%28p_%7Byj%7D%28%5Cxi_j%29%29%29)

Несколько примеров работы программы представлены ниже:

![naive_bayes_1.png](https://github.com/VladislavDuma/SMPR/blob/master/source/naive_bayes/naive_bayes_1.PNG)

![naive_bayes_2.png](https://github.com/VladislavDuma/SMPR/blob/master/source/naive_bayes/naive_bayes_2.PNG)

![naive_bayes_3.png](https://github.com/VladislavDuma/SMPR/blob/master/source/naive_bayes/naive_bayes_3.PNG)

Достоинства алгоритма:
1. Прост в реализации.
2. Низкие вычислительные затраты при обучении и классификации объектов.
3. Если признаки независимы, то алгоритм приближён к оптимальному.

Недостатки алгоритма:
1. Низкое качество классификации.

Работу программы с реализацией на Shiny (наивный байесовский классификатор) можно посмотреть [здесь](https://vladislav-duma.shinyapps.io/naive_bayes_wow/)

[К началу алгоритма (наивный байесовский классификатор)](#наивный-байесовский-классификатор)

[Вернуться к содержанию](#содержание)

### Plug-in ###

Алгоритм относится к нормальному дискриминантному анализу и применяется для многомерных (в данном случае - для многомерного нормального) распределений.
Решающее правило имеет вид:

![](http://latex.codecogs.com/gif.latex?a%28x%29%3Darg%5Cmax_%7By%5Cin%20Y%7D%5Clambda_y%5Chat%20P_y%5Chat%20p_y%28x%29),

где ![](http://latex.codecogs.com/gif.latex?%5Chat%20P_y%2C%5Chat%20p_y%28x%29) это восстановленная вероятность и плотность распределения класса.

![](http://latex.codecogs.com/gif.latex?%5Chat%20%5Cmu%28x%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Em_%7Bi%3D1%7Dx_i)

![](http://latex.codecogs.com/gif.latex?%5Chat%20p_y%28x%29%20%3D%20N%28x%2C%20%5Cmu%2C%20%5CSigma%29%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%28%282%5Cpi%29%5En%7C%5CSigma%7C%29%7D%5Cexp%28-%5Cfrac%7B1%7D%7B2%7D%28x-%5Cmu%29%5CSigma%5E%7B-1%7D%28x-%5Cmu%29%5ET%29)

где ![](http://latex.codecogs.com/gif.latex?%5CSigma) - восстановленная матрица ковариации.

Матрица ковариации вычисляется по формуле:  
![](http://latex.codecogs.com/gif.latex?%5CSigma%28x%2C%20%5Cmu%29%20%3D%20%5Cfrac%7B1%7D%7Bm-1%7D%5Csum%5Em_%7Bi%3D1%7D%28x_i-%5Cmu%29%28x_i-%5Cmu%29%5ET)

Восстановленную матрицу ковариации и вектор мат. ожидания подставляют в формулу плотности, которую подставляют в решающее правило.

Алгоритм позволяет построить разделяющую кривую (в данном случае - второго порядка) между классами, решив уравнение:

![](http://latex.codecogs.com/gif.latex?%5Clambda_1%5Chat%20P_1%5Chat%20p_1%28x%29%3D%5Clambda_2%5Chat%20P_2%20%5Chat%20p_2%28x%29)

Примеры работы программы:

![plug_1.png](https://github.com/VladislavDuma/SMPR/blob/master/img/bayes/plug_1.PNG)

![plug_2.png](https://github.com/VladislavDuma/SMPR/blob/master/img/bayes/plug_2.PNG)

![plug_3.png](https://github.com/VladislavDuma/SMPR/blob/master/img/bayes/plug_3.PNG)

![plug_4.png](https://github.com/VladislavDuma/SMPR/blob/master/img/bayes/plug_4.PNG)

Работу программы с реализацией на Shiny (Plug-in) можно посмотреть [здесь](https://vladislav-duma.shinyapps.io/bayes_classifiers_v2_wow/)

[К началу алгоритма (plug-in)](#plug-in)

[Вернуться к содержанию](#содержание)

### ЛДФ ###

ЛДФ является частным случаем подстановочного алгоритма, для случая, когда матрицы ковариации ![](http://latex.codecogs.com/gif.latex?%5CSigma) для всех классов равны.
В этом случае матрицу ковариации можно восстановить по формуле:

![](http://latex.codecogs.com/gif.latex?%5CSigma%28x%2C%20%5Cmu%29%20%3D%20%5Cfrac%7B1%7D%7Bm%20-%20%7CY%7C%7D%5Csum%5Em_%7Bi%3D1%7D%28x_i-%5Chat%20%5Cmu_%7Byi%7D%29%28x_i-%5Chat%20%5Cmu_%7Byi%7D%29%5ET)

где *|Y|* - мощность множества классов, *x* - объекты выборки.

Примеры работы программы:

![ldf_1.png](https://github.com/VladislavDuma/SMPR/blob/master/img/bayes/ldf_1.PNG)

![ldf_2.png](https://github.com/VladislavDuma/SMPR/blob/master/img/bayes/ldf_2.PNG)

![compare_1.png](https://github.com/VladislavDuma/SMPR/blob/master/img/bayes/compare_1.PNG)

Работу программы с реализацией на Shiny (ЛДФ) можно посмотреть [здесь](https://vladislav-duma.shinyapps.io/bayes_classifiers_v2_wow/)

[К началу алгоритма (ЛДФ)](#лдф)

[Вернуться к содержанию](#содержание)

### Сравнение plug-in и ЛДФ ###
Plug-in является более универсальным алгоритмом, но в некоторых частных случаях выгоднее использовать ЛДФ.

*ЛДФ*
![compare_3.png](https://github.com/VladislavDuma/SMPR/blob/master/img/bayes/compare_3.PNG)

*Plug-in*
![compare_plug_3.png](https://github.com/VladislavDuma/SMPR/blob/master/img/bayes/compare_plug_3.PNG)

Вывод: Plug-in является более универсальным алгоритмом классификации, нежели ЛДФ. Но есть частные случаи, где ЛДФ даёт гораздо лучший результат. Также, стоит отметить, что ЛДФ довольно прост в реализации и требует меньше вычислительных затрат, нежели Plug-in.

Работу программы с реализацией на Shiny (Plug-in и ЛДФ) можно посмотреть [здесь](https://vladislav-duma.shinyapps.io/bayes_classifiers_v2_wow/)

[К началу алгоритма (Cравнение plug-in и ЛДФ)](#сравнение-plug-in-и-ЛДФ)

[Вернуться к содержанию](#содержание)

## Линейные алгоритмы классификации ##

Рассмотрим задачу классификации с множеством объектов ![](http://latex.codecogs.com/gif.latex?X%3D%5Cmathbb%7BR%7D%5En) и множеством ответов *Y = {-1, +1}*.

Алгоритм ![](http://latex.codecogs.com/gif.latex?a%28x%2Cw%29%3Dsignf%28x%2Cw%29) является линейным алгоритмом классификации, где *w* - вектор параметров, а *f(x, w)* - дискриминантная функция. 

В случае, если *f(x, w) > 0*, то алгоритм отнесёт объект к классу *+1*, в ином случае - к классу *-1*.

Уравнение ![](http://latex.codecogs.com/gif.latex?%5Clangle%20w%2Cx%5Crangle%20%3D0) задаёт разделяющую классы гиперплоскость в
пространстве ![](http://latex.codecogs.com/gif.latex?%5Cmathbb%7BR%7D%5En).

### Стохастический градиент ###

Для подбора оптимального (минимизирующего эмпирический риск ![](http://latex.codecogs.com/gif.latex?Q%28w%2C%20X%5El%29%20%3D%20%5Csum%5El_%7Bi%3D1%7D%5Cpounds%20%28M%29) , где ![](http://latex.codecogs.com/gif.latex?%5Cpounds%20%28M%29) - функция потерь, *M* - отступ) значения вектора весов *w* будем пользоваться методом *стохастического градиента* — итерационный процесс, на каждом шаге которого сдвигаемся в сторону противоположную вектору градиента *Q′* до тех пор, пока вектор весов *w* не перестанет изменяться, причем вычисления градиента производится не на всех объектах обучения, а выбирается случайный объект (отсюда и название метода *«стохастический»*), на основе которого и происходят вычисления. В зависимости от функции потерь, которая используется в функционале эмпирического риска, будем получать различные линейные алгоритмы классификации.

При использовании метода стохастического градиента необходимо нормализовать исходные данные.

Метод работает следующим образом: выбирается начальное приближение вектора *w*, после чего запускается итерационный процесс, при котором на каждом шаге вектор *w* изменяется в направлении наиболее быстрого убывания *Q*. Это направление противоположно направлению вектора градиента ![](http://latex.codecogs.com/gif.latex?Q%27%3D%28%5Cfrac%7B%5Cpartial%20Q%28w%29%7D%7B%5Cpartial%20w_j%7D%29%5En_%7Bj%3D1%7D): ![](http://latex.codecogs.com/gif.latex?w%3Dw-%5Ceta%20Q%27%28w%29), где ![](http://latex.codecogs.com/gif.latex?%5Ceta) - *темп обучения*. 

Алгоритм:  
1. Инициализировать вектор *w*.
2. Вычислить начальное значение функционала ![](http://latex.codecogs.com/gif.latex?Q%3D%5Csum%5El_%7Bi%3D1%7D%5Cpounds%28%5Clangle%20w%2C%20x_i%5Crangle%20y_i%29).
3. Выбрать из выборки объект ![](http://latex.codecogs.com/gif.latex?x_i) случайным образом.
4. Вычислить ошибку алгоритма на выбранном объекте: ![](http://latex.codecogs.com/gif.latex?%5Cvarepsilon_i%20%3D%20%5Cpounds%20%28%5Clangle%20w%2C%20x_i%5Crangle%20y_i%29).
5. Сделать шаг градиентного спуска: ![](http://latex.codecogs.com/gif.latex?w%3Dw-%5Ceta%20%5Cpounds%5E%60%28%3Cw%2Cx_i%3Ey_i%29x_iy_i).  
6. Пересчитать значение эмпирического риска ![](http://latex.codecogs.com/gif.latex?Q%3D%281-%5Clambda%29Q&plus;%5Clambda%20%5Cvarepsilon).
7. Повторять пункты 3 - 6 до тех пор, пока значение *Q*  и/или вектор *w* не перестанут изменятся.

Достоинства:
1. Прост в реализации.
2. Метод легко обобщается и на нелинейные классификаторы и на нейронные сети.
3. Позволяет быстро настраивать веса даже на избыточно больших выборках.
4. Метод подходит для динамического обучения, когда данные поступают потоком, и вектор весов обновляется при появлении каждого объекта. 

Недостатки:
1. Неустойчивая сходимость (может сойтись к локальному экстремуму, может сходится очень медленно, а может вообще не сходится).
2. При большой размерности пространства ![](http://latex.codecogs.com/gif.latex?%5Cmathbb%7BR%7D%5En) или малой длине выборки возможно переобучение. Тогда классификация становится неустойчивой.
3. Если функция потерь имеет горизонтальные асимптоты, то процесс может попасть в состояние «паралича».

### ADALINE ###

*Алгоритм классификации ADALINE* — адаптивный линейный элемент, в качестве функции потерь используется квадратичная функция потерь.

Функция потерь является квадратичной: ![](http://latex.codecogs.com/gif.latex?%5Cpounds%28M%29%3D%28M-1%29%5E2)

Работу программы с реализацией на Shiny (Plug-in и ЛДФ) можно посмотреть [здесь](https://vladislav-duma.shinyapps.io/linear_classifiers/) (пункт - "ADALINE")

[К началу алгоритма (ADALINE)](#ADALINE)

[Вернуться к содержанию](#содержание)

### Персептрон Розенблатта ###

*Персептрон Розенблатта* — линейный классификатор, обучаемый с помощью стохастического градиента с правилом Хэбба и кусочно-линейной функции потерь.

Правило Хэбба: ![](http://latex.codecogs.com/gif.latex?if%20%28%5Clangle%20w%2Cx_i%5Crangle%20y_i%29%20%3C%200%20%5C%3Bthen%20%5C%3B%20w%3A%3Dw&plus;%5Ceta%20x_i%20y_i)

Функция потерь является кусочно-линейной: ![](http://latex.codecogs.com/gif.latex?%5Cpounds%20%28M%29%3D%28-M%29_%7B&plus;%7D)

Работу программы с реализацией на Shiny (Персептрон Розенблатта) можно посмотреть [здесь](https://vladislav-duma.shinyapps.io/linear_classifiers/) (пункт - "Персептрон Розенблатта") 

[К началу алгоритма (Персептрон Розенблатта)](#персептрон-розенблатта)

[Вернуться к содержанию](#содержание)

### Логистическая регрессия ###

*Логистическая регрессия* — линейный байесовский классификатор, использующий логарифмическую функцию потерь. Является одним из популярных алгоритмом классификации. Также можно выделить, что данный метод является одновременно и линейным, и байесовским классификатором.

Функция потерь имеет следующий вид: ![](http://latex.codecogs.com/gif.latex?%5Cpounds%28M%29%3D%5Clog_2%281&plus;e%5E%7B-M%7D%29)

Метод логистической регрессии основан на довольно сильных вероятностных предположениях, которые имеют несколько интересных последствий:

1. линейный классификатор оказывается оптимальным байесовским;
2. однозначно определяется функция потерь;
3. можно вычислять не только принадлежность объектов классам, но также получать и численные оценки вероятности их принадлежности.

Достоинства:

1.  Как правило,логистическая регрессия дает лучшие результаты по сравнению с линейным дискриминантом Фишера (поскольку она основана на менее жестких гипотезах), а также по сравнению с дельта-правилом и правилом Хэбба (поскольку она использует "более правильную" функцию потерь). 
2.  Возможность оценивать апостериорные вероятности и риски.

Недостатки:

1. Оценки вероятностей и рисков могут оказаться неадекватными, если не выполняются предположения теоремы. 
2.  Градиентный метод обучения логистической регрессии наследует все недостатки метода стохастического градиента. Практичная реализация должна предусматривать стандартизацию данных, отсев выбросов, регуляризацию (сокращение весов), отбор признаков, и другие эвристики для улучшения сходимости. Возможно применение метода второго порядка, но он требует обращения n×n-матриц на каждом шаге и также не застрахован от плохой сходимости.

Работу программы с реализацией на Shiny (Логистическая регрессия) можно посмотреть [здесь](https://vladislav-duma.shinyapps.io/linear_classifiers/) (пункт - "Логистич. регрессия")

[К началу алгоритма (Логистическая регрессия)](#логистическая-регрессия)

[Вернуться к содержанию](#содержание)

### Сравнение классификаторов ###

Сравним работу вышепредставленных алгоритмов - ADALINE, Персептрон Розенблатта и метод логистической регрессии.



Вывод: ADALINE и "Логистическая регрессия" гораздо точнее "правила Хэбба", когда выборка линейно разделима. Но когда выборка плохо разделима, или не разделима вовсе, ADALINE хуже "Логистической регрессии" и "правила Хэбба". Как итог, алгоритм стоит выбирать в зависимости от исходных данных.

Работу программы с реализацией на Shiny (Сравнение линейных классификторов) можно посмотреть [здесь](https://vladislav-duma.shinyapps.io/linear_classifiers/) (пункт - "Сравнение")

[К началу алгоритма (Логистическая регрессия)](#логистическая-регрессия)

[Вернуться к содержанию](#содержание)
